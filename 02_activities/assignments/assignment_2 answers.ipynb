{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Adult* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult). Extract the data files into the subdirectory: `../05_src/data/adult/` (relative to `./05_src/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 2 Nov 1 2200h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../05_src/data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'native-country', 'income'\n",
    "]\n",
    "adult_dt = (pd.read_csv('../../05_src/data/adult/adult.test', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n",
    "\n",
    "adult_dt = (pd.read_csv('../../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data:\n",
    "\n",
    "+ Create a dataframe `X` that holds the features (all columns that are not `income`).\n",
    "+ Create a dataframe `Y` that holds the target data (`income`).\n",
    "+ From `X` and `Y`, obtain the training and testing data sets:\n",
    "\n",
    "    - Use a train-test split of 70-30%. \n",
    "    - Set the random state of the splitting function to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shapes:\n",
      "X_train: (22792, 14)\n",
      "Y_train: (22792,)\n",
      "\n",
      "Testing set shapes:\n",
      "X_test: (9769, 14)\n",
      "Y_test: (9769,)\n",
      "\n",
      "First few rows of X_train:\n",
      "       age          workclass  fnlwgt      education  education-num  \\\n",
      "19749   34   Self-emp-not-inc   56460        HS-grad              9   \n",
      "1216    48   Self-emp-not-inc  243631   Some-college             10   \n",
      "27962   23          State-gov   56402   Some-college             10   \n",
      "23077   56          Local-gov  255406        HS-grad              9   \n",
      "10180   17            Private  297246           11th              7   \n",
      "\n",
      "            marital-status        occupation    relationship  \\\n",
      "19749   Married-civ-spouse   Farming-fishing            Wife   \n",
      "1216    Married-civ-spouse      Craft-repair         Husband   \n",
      "27962   Married-civ-spouse    Prof-specialty         Husband   \n",
      "23077             Divorced   Exec-managerial   Not-in-family   \n",
      "10180        Never-married   Priv-house-serv       Own-child   \n",
      "\n",
      "                      race      sex  capital-gain  capital-loss  \\\n",
      "19749                White   Female             0          2179   \n",
      "1216    Amer-Indian-Eskimo     Male          7688             0   \n",
      "27962                White     Male             0             0   \n",
      "23077                White   Female             0             0   \n",
      "10180                White   Female             0             0   \n",
      "\n",
      "       hours-per-week  native-country  \n",
      "19749              12   United-States  \n",
      "1216               40   United-States  \n",
      "27962              30   United-States  \n",
      "23077              40   United-States  \n",
      "10180               9   United-States  \n",
      "\n",
      "First few rows of Y_train:\n",
      "19749    0\n",
      "1216     1\n",
      "27962    0\n",
      "23077    0\n",
      "10180    0\n",
      "Name: income, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# Create X and Y dataframes\n",
    "X = adult_dt.drop('income', axis=1)\n",
    "Y = adult_dt['income']\n",
    "\n",
    "# Split data into training and testing sets 70:30 with a random state of 42\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, \n",
    "    test_size=0.3,   \n",
    "    random_state=42   \n",
    ")\n",
    "\n",
    "# Print shapes of datasets\n",
    "print(\"Training set shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"Y_train: {Y_train.shape}\")\n",
    "print(\"\\nTesting set shapes:\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"Y_test: {Y_test.shape}\")\n",
    "\n",
    "# Print first few rows of X_train and Y_train\n",
    "print(\"\\nFirst few rows of X_train:\")\n",
    "print(X_train.head())\n",
    "print(\"\\nFirst few rows of Y_train:\")\n",
    "print(Y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random States\n",
    "\n",
    "Please comment: \n",
    "\n",
    "+ What is the [random state](https://scikit-learn.org/stable/glossary.html#term-random_state) of the [splitting function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)? \n",
    "+ Why is it [useful](https://en.wikipedia.org/wiki/Reproducibility)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the random state of the splitting function?\n",
    "The random state in the splitting function is a number that controls how the data is split into the training and testing sets. When this function is applied, the data is randomly mixed up before it is split. By setting the random_state to a specific number, you make sure that every time you run the code, the same split of the data is produced.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it useful?\n",
    "Using a fixed random_state is helpful for the following reasons:\n",
    "\n",
    "1/ Consistency in that it ensures that you get the same results every time you run your code. This is important when you want to compare how different models perform under the same conditions.\n",
    "\n",
    "2/ Debugging: When working on models, having the same data split helps to identify changes made to improve the model rather than just changing the data it trains on.\n",
    "\n",
    "3/ Collaboration - Using a fixed random_state allows allows for improved collaboration in that the random state is set across collaborators, which is important for teamwork and verifying each other's work.\n",
    "\n",
    "4/ Research and Publication: In research, being able to repeat experiments and get the same results is very important. Using a fixed random_state helps others replicate your work and confirm your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create a [Column Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) that treats the features as follows:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * Apply [KNN-based imputation for completing missing values](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html):\n",
    "        \n",
    "        + Consider the 7 nearest neighbours.\n",
    "        + Weight each neighbour by the inverse of its distance, causing closer neigbours to have more influence than more distant ones.\n",
    "    * [Scale features using statistics that are robust to outliers](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler).\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * Apply a [simple imputation strategy](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer):\n",
    "\n",
    "        + Use the most frequent value to complete missing values, also called the *mode*.\n",
    "\n",
    "    * Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):\n",
    "        \n",
    "        + Handle unknown labels if they exist.\n",
    "        + Drop one column for binary variables.\n",
    "    \n",
    "    \n",
    "The column transformer should look like this:\n",
    "\n",
    "![](./images/assignment_2__column_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Transformer Structure:\n",
      "ColumnTransformer(transformers=[('num',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  KNNImputer(n_neighbors=7,\n",
      "                                                             weights='distance')),\n",
      "                                                 ('scaler', RobustScaler())]),\n",
      "                                 ['age', 'fnlwgt', 'education-num',\n",
      "                                  'capital-gain', 'capital-loss',\n",
      "                                  'hours-per-week']),\n",
      "                                ('cat',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent')),\n",
      "                                                 ('encoder',\n",
      "                                                  OneHotEncoder(drop='if_binary',\n",
      "                                                                handle_unknown='ignore'))]),\n",
      "                                 ['workclass', 'education', 'marital-status',\n",
      "                                  'occupation', 'relationship', 'race', 'sex',\n",
      "                                  'native-country'])])\n",
      "\n",
      "Numerical Features:\n",
      "['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
      "\n",
      "Categorical Features:\n",
      "['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', \n",
    "                     'capital-loss', 'hours-per-week']\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                       'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "# Create numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', KNNImputer(n_neighbors=7, weights='distance')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# Create categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='if_binary'))\n",
    "])\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Print structure of preprocessor\n",
    "print(\"Column Transformer Structure:\")\n",
    "print(preprocessor)\n",
    "\n",
    "# Print feature lists to verify correct column assignment\n",
    "print(\"\\nNumerical Features:\")\n",
    "print(numerical_features)\n",
    "print(\"\\nCategorical Features:\")\n",
    "print(categorical_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `classifier` and assign a [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to it.\n",
    "\n",
    "The pipeline looks like this:\n",
    "\n",
    "![](./images/assignment_2__pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Structure:\n",
      "Pipeline(steps=[('preprocessing',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   KNNImputer(n_neighbors=7,\n",
      "                                                                              weights='distance')),\n",
      "                                                                  ('scaler',\n",
      "                                                                   RobustScaler())]),\n",
      "                                                  ['age', 'fnlwgt',\n",
      "                                                   'education-num',\n",
      "                                                   'capital-gain',\n",
      "                                                   'capital-loss',\n",
      "                                                   'hours-per-week']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('encoder',\n",
      "                                                                   OneHotEncoder(drop='if_binary',\n",
      "                                                                                 handle_unknown='ignore'))]),\n",
      "                                                  ['workclass', 'education',\n",
      "                                                   'marital-status',\n",
      "                                                   'occupation', 'relationship',\n",
      "                                                   'race', 'sex',\n",
      "                                                   'native-country'])])),\n",
      "                ('classifier', RandomForestClassifier())])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Print pipeline structure  \n",
    "print(\"Pipeline Structure:\")\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Evaluate the model pipeline using [`cross_validate()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html):\n",
    "\n",
    "+ Measure the following [preformance metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values): negative log loss, ROC AUC, accuracy, and balanced accuracy.\n",
    "+ Report the training and validation results. \n",
    "+ Use five folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results:\n",
      "\n",
      "neg_log_loss:\n",
      "  Training:   mean = -0.082 (+/- 0.001)\n",
      "  Validation: mean = -0.378 (+/- 0.045)\n",
      "\n",
      "roc_auc:\n",
      "  Training:   mean = 1.000 (+/- 0.000)\n",
      "  Validation: mean = 0.903 (+/- 0.004)\n",
      "\n",
      "accuracy:\n",
      "  Training:   mean = 1.000 (+/- 0.000)\n",
      "  Validation: mean = 0.853 (+/- 0.007)\n",
      "\n",
      "balanced_accuracy:\n",
      "  Training:   mean = 1.000 (+/- 0.000)\n",
      "  Validation: mean = 0.776 (+/- 0.006)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, log_loss, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'neg_log_loss': 'neg_log_loss',\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'accuracy': 'accuracy',\n",
    "    'balanced_accuracy': 'balanced_accuracy'\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(\n",
    "    pipe,\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    scoring=scoring,\n",
    "    cv=5,                # 5 folds\n",
    "    return_train_score=True,  # Get both training and validation scores\n",
    "    n_jobs=-1           # Use all available cores\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Cross-validation results:\")\n",
    "for metric in scoring.keys():\n",
    "    train_scores = cv_results[f'train_{metric}']\n",
    "    test_scores = cv_results[f'test_{metric}']\n",
    "    \n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  Training:   mean = {train_scores.mean():.3f} (+/- {train_scores.std() * 2:.3f})\")\n",
    "    print(f\"  Validation: mean = {test_scores.mean():.3f} (+/- {test_scores.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the fold-level results as a pandas data frame and sorted by negative log loss of the test (validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results sorted by test negative log loss:\n",
      "      train_neg_log_loss  test_neg_log_loss  train_roc_auc  test_roc_auc  \\\n",
      "Fold                                                                       \n",
      "3              -0.081471          -0.391009            1.0      0.900593   \n",
      "4              -0.082184          -0.378408            1.0      0.906683   \n",
      "5              -0.081660          -0.373080            1.0      0.902779   \n",
      "2              -0.081000          -0.361883            1.0      0.902086   \n",
      "1              -0.081721          -0.336490            1.0      0.904355   \n",
      "\n",
      "      train_accuracy  test_accuracy  train_balanced_accuracy  \\\n",
      "Fold                                                           \n",
      "3           1.000000       0.849276                 1.000000   \n",
      "4           1.000000       0.860465                 1.000000   \n",
      "5           0.999945       0.856297                 0.999887   \n",
      "2           1.000000       0.846458                 1.000000   \n",
      "1           1.000000       0.849528                 1.000000   \n",
      "\n",
      "      test_balanced_accuracy  \n",
      "Fold                          \n",
      "3                   0.770677  \n",
      "4                   0.785139  \n",
      "5                   0.775003  \n",
      "2                   0.765588  \n",
      "1                   0.772838  \n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame from cross-validation results\n",
    "results_df = pd.DataFrame({\n",
    "    'train_neg_log_loss': cv_results['train_neg_log_loss'],\n",
    "    'test_neg_log_loss': cv_results['test_neg_log_loss'],\n",
    "    'train_roc_auc': cv_results['train_roc_auc'],\n",
    "    'test_roc_auc': cv_results['test_roc_auc'],\n",
    "    'train_accuracy': cv_results['train_accuracy'],\n",
    "    'test_accuracy': cv_results['test_accuracy'],\n",
    "    'train_balanced_accuracy': cv_results['train_balanced_accuracy'],\n",
    "    'test_balanced_accuracy': cv_results['test_balanced_accuracy']\n",
    "})\n",
    "\n",
    "# Add fold number column\n",
    "results_df.index.name = 'Fold'\n",
    "results_df.index += 1  # Start fold numbering at 1\n",
    "\n",
    "# Sort by test neg_log_loss\n",
    "results_df_sorted = results_df.sort_values('test_neg_log_loss', ascending=True)\n",
    "\n",
    "# Display sorted results\n",
    "print(\"Cross-validation results sorted by test negative log loss:\")\n",
    "print(results_df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean scores across all folds:\n",
      "\n",
      "Training Metrics:\n",
      "-----------------\n",
      "neg_log_loss         -0.082\n",
      "roc_auc              1.000\n",
      "accuracy             1.000\n",
      "balanced_accuracy    1.000\n",
      "\n",
      "Validation Metrics:\n",
      "------------------\n",
      "neg_log_loss         -0.368\n",
      "roc_auc              0.903\n",
      "accuracy             0.852\n",
      "balanced_accuracy    0.774\n"
     ]
    }
   ],
   "source": [
    "# Calculate means for all metrics\n",
    "mean_scores = results_df.mean()\n",
    "\n",
    "# Format display of means\n",
    "print(\"Mean scores across all folds:\")\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(\"-----------------\")\n",
    "for metric in mean_scores.index:\n",
    "    if metric.startswith('train_'):\n",
    "        print(f\"{metric[6:]:20} {mean_scores[metric]:.3f}\")\n",
    "\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(\"------------------\")\n",
    "for metric in mean_scores.index:\n",
    "    if metric.startswith('test_'):\n",
    "        print(f\"{metric[5:]:20} {mean_scores[metric]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the same performance metrics (negative log loss, ROC AUC, accuracy, and balanced accuracy) using the testing data `X_test` and `Y_test`. Display results as a dictionary.\n",
    "\n",
    "*Tip*: both, `roc_auc()` and `neg_log_loss()` will require prediction scores from `pipe.predict_proba()`. However, for `roc_auc()` you should only pass the last column `Y_pred_proba[:, 1]`. Use `Y_pred_proba` with `neg_log_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set performance metrics:\n",
      "{'negative_log_loss': 0.5228202702042561, 'roc_auc': 0.6765806883942665, 'accuracy': 0.7629235336267786, 'balanced_accuracy': 0.500163875792931}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Q\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 4, 5, 7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\Q\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 4, 5, 7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "Y_pred_proba = pipe.predict_proba(X_test)\n",
    "Y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Calculate metrics\n",
    "neg_log_loss_value = log_loss(Y_test, Y_pred_proba)\n",
    "roc_auc_value = roc_auc_score(Y_test, Y_pred_proba[:, 1])\n",
    "accuracy_value = accuracy_score(Y_test, Y_pred)\n",
    "balanced_accuracy_value = balanced_accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "# Create results dictionary\n",
    "test_metrics = {\n",
    "    'negative_log_loss': neg_log_loss_value,\n",
    "    'roc_auc': roc_auc_value,\n",
    "    'accuracy': accuracy_value,\n",
    "    'balanced_accuracy': balanced_accuracy_value\n",
    "}\n",
    "\n",
    "# Display results\n",
    "print(\"Test set performance metrics:\")\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Recoding\n",
    "\n",
    "In the first code chunk of this document, we loaded the data and immediately recoded the target variable `income`. Why is this [convenient](https://scikit-learn.org/stable/modules/model_evaluation.html#binary-case)?\n",
    "\n",
    "The specific line was:\n",
    "\n",
    "```\n",
    "adult_dt = (pd.read_csv('../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ Simplifies Data Processing: By converting the income column to a binary format (0 and 1), it becomes easier to work with the data for ML models. Most algorithms require numerical input, so having income as 0 (for less than or equal to $50K) and 1 (for greater than $50K) simplifies the data preparation process\n",
    "\n",
    "2/ Clarity: Recoding the target variable right after loading the data provides clarity on what the target variable represents. The code is easier to understand, i.e. now that income is now a binary variable \n",
    "\n",
    "3/ Consistency: When evaluating model performance, having a binary target variable allows for straightforward calculations of metrics like accuracy, precision, recall, and F1 score, which enables the metrics to become easier to compute and interpret when the target variable is in a binary format.\n",
    "\n",
    "4/ Facilitates Data Exploration: With the target variable recoded, it becomes more efficient to explore relationships between features and the target. Creating visualizations or perform statistical analyses becomes easier to see how different features relate to income levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_2_rubric_clean.xlsx) contains the criteria for assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Becker,Barry and Kohavi,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
