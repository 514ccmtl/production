{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with parquet files\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ In this assignment, we will use the data downloaded with the module `data_manager` to create features.\n",
    "\n",
    "(11 pts total)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ This notebook assumes that price data is available to you in the environment variable `PRICE_DATA`. If you have not done so, then execute the notebook `production_2_data_engineering.ipynb` to create this data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variables using dotenv. (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 answers Oct 28 0038h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Q\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:17: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 11.0.0. Please consider upgrading.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Q\\AppData\\Local\\Temp\\ipykernel_15184\\2156611817.py:5: DeprecationWarning: The current Dask DataFrame implementation is deprecated. \n",
      "In a future release, Dask DataFrame will use new implementation that\n",
      "contains several improvements including a logical query planning.\n",
      "The user-facing DataFrame API will remain unchanged.\n",
      "\n",
      "The new implementation is already available and can be enabled by\n",
      "installing the dask-expr library:\n",
      "\n",
      "    $ pip install dask-expr\n",
      "\n",
      "and turning the query planning option on:\n",
      "\n",
      "    >>> import dask\n",
      "    >>> dask.config.set({'dataframe.query-planning': True})\n",
      "    >>> import dask.dataframe as dd\n",
      "\n",
      "API documentation for the new implementation is available at\n",
      "https://docs.dask.org/en/stable/dask-expr-api.html\n",
      "\n",
      "Any feedback can be reported on the Dask issue tracker\n",
      "https://github.com/dask/dask/issues \n",
      "\n",
      "  import dask.dataframe as dd\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from glob import glob\n",
    "import yfinance as yf\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getenv('SRC_DIR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dask DataFrame\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variable `PRICE_DATA`.\n",
    "+ Use [glob](https://docs.python.org/3/library/glob.html) to find the path of all parquet files in the directory `PRICE_DATA`.\n",
    "\n",
    "(1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No parquet files found in the PRICE_DATA directory.\n"
     ]
    }
   ],
   "source": [
    "# Load PRICE_DATA environment variable\n",
    "PRICE_DATA = os.getenv(\"PRICE_DATA\")\n",
    "\n",
    "if PRICE_DATA:\n",
    "    # Use glob to find all parquet files in PRICE_DATA directory \n",
    "    parquet_files = glob(os.path.join(PRICE_DATA, \"**/*.parquet\"), recursive=True)\n",
    "    \n",
    "    # Filter out directories  \n",
    "    parquet_files = [f for f in parquet_files if os.path.isfile(f)]\n",
    "    \n",
    "    if parquet_files:\n",
    "        print(f\"Found {len(parquet_files)} parquet files.\")\n",
    "        print(\"First few file paths:\")\n",
    "        for file in parquet_files[:5]:  # Print first 5 file paths  \n",
    "            print(file)\n",
    "        \n",
    "        # Load parquet files into Dask df\n",
    "        dd_px = dd.read_parquet(parquet_files)\n",
    "        \n",
    "        # Check if 'ticker' column exists before setting it as index\n",
    "        if 'ticker' in dd_px.columns:\n",
    "            dd_px = dd_px.set_index(\"ticker\")\n",
    "        \n",
    "        print(\"\\nDask dataframe created successfully.\")\n",
    "        print(f\"Columns: {dd_px.columns.tolist()}\")\n",
    "        print(f\"Number of partitions: {dd_px.npartitions}\")\n",
    "    else:\n",
    "        print(\"No parquet files found in the PRICE_DATA directory.\")\n",
    "else:\n",
    "    print(\"PRICE_DATA environment variable not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ticker and using Dask, do the following:\n",
    "\n",
    "+ Add lags for variables Close and Adj_Close.\n",
    "+ Add returns based on Adjusted Close:\n",
    "    \n",
    "    - `returns`: (Adj Close / Adj Close_lag) - 1\n",
    "\n",
    "+ Add the following range: \n",
    "\n",
    "    - `hi_lo_range`: this is the day's High minus Low.\n",
    "\n",
    "+ Assign the result to `dd_feat`.\n",
    "\n",
    "(4 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dd_px' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Apply function to each ticker group\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m dd_feat \u001b[38;5;241m=\u001b[39m \u001b[43mdd_px\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(process_ticker, meta\u001b[38;5;241m=\u001b[39mdd_px)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures added successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns in dd_feat: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdd_feat\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dd_px' is not defined"
     ]
    }
   ],
   "source": [
    "def process_ticker(group):\n",
    "    \n",
    "    # Add lags for Close and Adj Close\n",
    "    group['Close_lag'] = group['Close'].shift(1)\n",
    "    group['Adj Close_lag'] = group['Adj Close'].shift(1)\n",
    "    \n",
    "    # Calculate returns based on Adjusted Close\n",
    "    group['returns'] = (group['Adj Close'] / group['Adj Close_lag']) - 1\n",
    "    \n",
    "    # Calculate high-low range\n",
    "    group['hi_lo_range'] = group['High'] - group['Low']\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply function to each ticker group\n",
    "dd_feat = dd_px.groupby('ticker').apply(process_ticker, meta=dd_px)\n",
    "\n",
    "print(\"Features added successfully.\")\n",
    "print(f\"Columns in dd_feat: {dd_feat.columns.tolist()}\")\n",
    "print(f\"Number of partitions: {dd_feat.npartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convert the Dask data frame to a pandas data frame. \n",
    "+ Add a rolling average return calculation with a window of 10 days.\n",
    "+ *Tip*: Consider using `.rolling(10).mean()`.\n",
    "\n",
    "(3 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dd_feat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check type of dd_feat\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of dd_feat:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(\u001b[43mdd_feat\u001b[49m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print current columns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCurrent columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dd_feat' is not defined"
     ]
    }
   ],
   "source": [
    "# Check type of dd_feat\n",
    "print(\"Type of dd_feat:\", type(dd_feat))\n",
    "\n",
    "# Print current columns\n",
    "print(\"\\nCurrent columns:\")\n",
    "print(dd_feat.columns)\n",
    "\n",
    "# Convert Dask DataFrame to pandas DataFrame\n",
    "if isinstance(dd_feat, dd.DataFrame):\n",
    "    df_feat = dd_feat.compute()\n",
    "else:\n",
    "    df_feat = dd_feat  \n",
    "\n",
    "# Sort the DataFrame by ticker and date\n",
    "df_feat = df_feat.sort_values(['ticker', 'Date'])\n",
    "\n",
    "# Set 'ticker' as index for groupby operation\n",
    "df_feat = df_feat.set_index('ticker')\n",
    "\n",
    "# Add 10-day rolling average return calculation window\n",
    "df_feat['rolling_avg_return'] = df_feat.groupby('ticker')['returns'].rolling(window=10).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Print information of resulting DataFrame\n",
    "print(\"\\nRolling average return added successfully.\")\n",
    "print(f\"Shape of df_feat: {df_feat.shape}\")\n",
    "print(f\"Columns in df_feat: {df_feat.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows of DataFrame\n",
    "print(\"\\nFirst few rows of df_feat:\")\n",
    "print(df_feat.head())\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df_feat.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment:\n",
    "\n",
    "+ Was it necessary to convert to pandas to calculate the moving average return?\n",
    "+ Would it have been better to do it in Dask? Why?\n",
    "\n",
    "(1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Was it necessary to convert to pandas to calculate the moving average return?\n",
    "No, it wasn't strictly necessary. Dask provides functionality for rolling operations including moving averages, which could have been used directly on the Dask DataFrame.\n",
    "\n",
    "2. Would it have been better to do it in Dask? Why?\n",
    "It would have been better to use Dask for a number of reasons, including the following:\n",
    "\n",
    "Scalability: Dask is designed to handle larger-than-memory datasets. If the dataset is very large, Dask can process it in chunks without loading everything into memory at once.\n",
    "\n",
    "Parallel processing: Dask can leverage multiple cores or even a cluster of machines to perform computations in parallel, potentially speeding up the calculation for large datasets.\n",
    "\n",
    "Lazy evaluation: Dask uses lazy evaluation, which means it builds up a task graph of operations and only executes when necessary, which makes running the computation more efficient especially for complex chains of operations.\n",
    "\n",
    "Consistency: Keeping the data in Dask format throughout the pipeline maintains consistency and allows for easier integration with other Dask operations.\n",
    "\n",
    "Large datasets: Dask can be more useful and run more efficiently for large datasets\n",
    "\n",
    "In this specific case, given that we're dealing with financial time series data that could potentially be quite large, it would likely have been better to perform the rolling average calculation in Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_1_rubric_clean.xlsx) contains the criteria for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-1`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
