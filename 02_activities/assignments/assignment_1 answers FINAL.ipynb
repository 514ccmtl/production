{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with parquet files\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ In this assignment, we will use the data downloaded with the module `data_manager` to create features.\n",
    "\n",
    "(11 pts total)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ This notebook assumes that price data is available to you in the environment variable `PRICE_DATA`. If you have not done so, then execute the notebook `production_2_data_engineering.ipynb` to create this data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variables using dotenv. (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 answers Nov 1 2200h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from glob import glob\n",
    "import yfinance as yf\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.getenv('SRC_DIR'))\n",
    "\n",
    "# Load Dask DataFrame\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variable `PRICE_DATA`.\n",
    "+ Use [glob](https://docs.python.org/3/library/glob.html) to find the path of all parquet files in the directory `PRICE_DATA`.\n",
    "\n",
    "(1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet files found: []\n",
      "Price                       Date  Adj Close      Close       High        Low  \\\n",
      "ticker                                                                         \n",
      "A      2000-01-03 00:00:00+00:00  43.463020  51.502148  56.464592  48.193848   \n",
      "A      2000-01-04 00:00:00+00:00  40.142944  47.567955  49.266811  46.316166   \n",
      "A      2000-01-05 00:00:00+00:00  37.652874  44.617310  47.567955  43.141991   \n",
      "A      2000-01-06 00:00:00+00:00  36.219196  42.918453  44.349072  41.577251   \n",
      "A      2000-01-07 00:00:00+00:00  39.237442  46.494991  47.165592  42.203148   \n",
      "\n",
      "Price        Open   Volume       sector                       subsector  year  \\\n",
      "ticker                                                                          \n",
      "A       56.330471  4674353  Health Care  Life Sciences Tools & Services  2000   \n",
      "A       48.730328  4765083  Health Care  Life Sciences Tools & Services  2000   \n",
      "A       47.389126  5758642  Health Care  Life Sciences Tools & Services  2000   \n",
      "A       44.080830  2534434  Health Care  Life Sciences Tools & Services  2000   \n",
      "A       42.247852  2819626  Health Care  Life Sciences Tools & Services  2000   \n",
      "\n",
      "Price   Adj_Close  \n",
      "ticker             \n",
      "A       51.502148  \n",
      "A       47.567955  \n",
      "A       44.617310  \n",
      "A       42.918453  \n",
      "A       46.494991  \n",
      "Index(['Date', 'Adj Close', 'Close', 'High', 'Low', 'Open', 'Volume', 'sector',\n",
      "       'subsector', 'year', 'Adj_Close'],\n",
      "      dtype='object', name='Price')\n",
      "The 'Price ticker' column is missing.\n"
     ]
    }
   ],
   "source": [
    "ft_path = os.getenv(\"PRICE_DATA\")\n",
    "# df_raw = pd.read_csv(ft_path)\n",
    "# os.makedirs(ft_path, exist_ok=True)\n",
    "\n",
    "# Find parquet files\n",
    "parquet_files = glob(os.path.join(ft_path, \"*/path.o.parquet\"), recursive=True)\n",
    "\n",
    "# Print list of parquet files found\n",
    "print(\"Parquet files found:\", parquet_files)\n",
    "\n",
    "# Print head and columns of DataFrame\n",
    "print(dd_price_data.head())\n",
    "print(dd_price_data.columns)\n",
    "\n",
    "# Strip whitespace from column names\n",
    "dd_price_data.columns = dd_price_data.columns.str.strip()\n",
    "\n",
    "# Check if the 'Price ticker' column exists\n",
    "if 'ticker' in dd_price_data.columns:\n",
    "    print(\"The 'Price ticker' column is present.\")\n",
    "else:\n",
    "    print(\"The 'Price ticker' column is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ticker and using Dask, do the following:\n",
    "\n",
    "+ Add lags for variables Close and Adj_Close.\n",
    "+ Add returns based on Adjusted Close:\n",
    "    \n",
    "    - `returns`: (Adj Close / Adj Close_lag) - 1\n",
    "\n",
    "+ Add the following range: \n",
    "\n",
    "    - `hi_lo_range`: this is the day's High minus Low.\n",
    "\n",
    "+ Assign the result to `dd_feat`.\n",
    "\n",
    "(4 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Q\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pyarrow\\pandas_compat.py:766: DeprecationWarning: DatetimeTZBlock is deprecated and will be removed in a future version. Use public APIs instead.\n",
      "  klass=_int.DatetimeTZBlock,\n",
      "c:\\Users\\Q\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\core\\frame.py:717: DeprecationWarning: Passing a BlockManager to DataFrame is deprecated and will raise in a future version. Use public APIs instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Missing required column: ticker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 46\u001b[0m\n\u001b[0;32m     32\u001b[0m dd_feat \u001b[38;5;241m=\u001b[39m dd_price_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(process_ticker, meta\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime64[ns]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice ticker\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi_lo_range\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     43\u001b[0m })\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Show sample of the result \u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdd_feat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Q\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask\\base.py:377\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Q\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask\\base.py:663\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    660\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 663\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\Q\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask\\dataframe\\groupby.py:215\u001b[0m, in \u001b[0;36m_groupby_slice_apply\u001b[1;34m(df, grouper, key, func, group_keys, dropna, observed, *args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key:\n\u001b[0;32m    214\u001b[0m     g \u001b[38;5;241m=\u001b[39m g[key]\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g\u001b[38;5;241m.\u001b[39mapply(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[28], line 17\u001b[0m, in \u001b[0;36mprocess_ticker\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m required_columns:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 17\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required column: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create lagged variables for Close and Adj_Close\u001b[39;00m\n\u001b[0;32m     20\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose_lag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Missing required column: ticker'"
     ]
    }
   ],
   "source": [
    "# Load the data from Parquet files\n",
    "dd_price_data = dd.read_parquet(r\"..\\..\\05_src\\data\\prices\")\n",
    "\n",
    "# Check if 'Adj_Close' exists, if not, create it\n",
    "if 'Adj_Close' not in dd_price_data.columns:\n",
    "    if 'Close' in dd_price_data.columns:\n",
    "        dd_price_data['Adj_Close'] = dd_price_data['Close']  # Adjust as needed\n",
    "    else:\n",
    "        raise KeyError(\"Missing required column: Close to create Adj_Close\")\n",
    "\n",
    "# Define a function to process each ticker separately\n",
    "def process_ticker(data):\n",
    "    # Check if required columns exist\n",
    "    required_columns = ['Close', 'Adj_Close', 'High', 'Low', 'ticker']\n",
    "    for col in required_columns:\n",
    "        if col not in data.columns:\n",
    "            raise KeyError(f\"Missing required column: {col}\")\n",
    "\n",
    "    # Create lagged variables for Close and Adj_Close\n",
    "    data['Close_lag'] = data['Close'].shift(1)\n",
    "    data['Adj_Close_lag'] = data['Adj_Close'].shift(1)\n",
    "    \n",
    "    # Calculate returns based on Adj_Close\n",
    "    data['returns'] = (data['Adj_Close'] / data['Adj_Close_lag']) - 1\n",
    "    \n",
    "    # Calculate hi_lo_range \n",
    "    data['hi_lo_range'] = data['High'] - data['Low']\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Group by 'ticker' and apply the function\n",
    "dd_feat = dd_price_data.groupby('ticker').apply(process_ticker, meta={\n",
    "    'Date': 'datetime64[ns]',\n",
    "    'Price ticker': 'object',\n",
    "    'Close': 'float64',\n",
    "    'Adj_Close': 'float64',\n",
    "    'Close_lag': 'float64',\n",
    "    'Adj_Close_lag': 'float64',\n",
    "    'returns': 'float64',\n",
    "    'High': 'float64',\n",
    "    'Low': 'float64',\n",
    "    'hi_lo_range': 'float64'\n",
    "})\n",
    "\n",
    "# Show sample of the result \n",
    "print(dd_feat.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convert the Dask data frame to a pandas data frame. \n",
    "+ Add a rolling average return calculation with a window of 10 days.\n",
    "+ *Tip*: Consider using `.rolling(10).mean()`.\n",
    "\n",
    "(3 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Missing required column: ticker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert to pandas DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_pandas \u001b[38;5;241m=\u001b[39m \u001b[43mdd_feat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Add a rolling average return calculation with a 10-day window\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df_pandas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrolling_avg_return\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_pandas[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mrolling(window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\Q\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask\\base.py:377\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Q\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask\\base.py:663\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    660\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 663\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\Q\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\dask\\dataframe\\groupby.py:215\u001b[0m, in \u001b[0;36m_groupby_slice_apply\u001b[1;34m(df, grouper, key, func, group_keys, dropna, observed, *args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key:\n\u001b[0;32m    214\u001b[0m     g \u001b[38;5;241m=\u001b[39m g[key]\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g\u001b[38;5;241m.\u001b[39mapply(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m, in \u001b[0;36mprocess_ticker\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m required_columns:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 17\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required column: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Create lagged variables for Close and Adj_Close\u001b[39;00m\n\u001b[0;32m     20\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose_lag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Missing required column: ticker'"
     ]
    }
   ],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df_pandas = dd_feat.compute()\n",
    "\n",
    "# Add a rolling average return calculation with a 10-day window\n",
    "df_pandas['rolling_avg_return'] = df_pandas['returns'].rolling(window=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment:\n",
    "\n",
    "+ Was it necessary to convert to pandas to calculate the moving average return?\n",
    "+ Would it have been better to do it in Dask? Why?\n",
    "\n",
    "(1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Was it necessary to convert to pandas to calculate the moving average return?\n",
    "# No, it wasn't strictly necessary. Dask provides functionality for rolling operations including moving averages, which could have been used directly on the Dask DataFrame.\n",
    "\n",
    "2. Would it have been better to do it in Dask? Why?\n",
    "# It would have been better to use Dask for a number of reasons, including the following:\n",
    "\n",
    "# Scalability: Dask is designed to handle larger-than-memory datasets. If the dataset is very large, Dask can process it in chunks without loading everything into memory at once.\n",
    "\n",
    "# Parallel processing: Dask can leverage multiple cores or even a cluster of machines to perform computations in parallel, potentially speeding up the calculation for large datasets.\n",
    "\n",
    "# Lazy evaluation: Dask uses lazy evaluation, which means it builds up a task graph of operations and only executes when necessary, which makes running the computation more efficient especially for complex chains of operations.\n",
    "\n",
    "# Consistency: Keeping the data in Dask format throughout the pipeline maintains consistency and allows for easier integration with other Dask operations.\n",
    "\n",
    "# Large datasets: Dask can be more useful and run more efficiently for large datasets\n",
    "\n",
    "# In this specific case, given that we're dealing with financial time series data that could potentially be quite large, it would likely have been better to perform the rolling average calculation in Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_1_rubric_clean.xlsx) contains the criteria for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-1`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
